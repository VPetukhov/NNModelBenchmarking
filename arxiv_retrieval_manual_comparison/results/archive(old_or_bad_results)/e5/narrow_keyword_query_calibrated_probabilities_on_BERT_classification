1: 
Beyond temperature scaling: Obtaining well-calibrated multiclass
  probabilities with Dirichlet calibration
  Class probabilities predicted by most multiclass classifiers are
uncalibrated, often tending towards over-confidence. With neural networks,
calibration can be improved by temperature scaling, a method to learn a single
corrective multiplicative factor for inputs to the last softmax layer. On
non-neural models the existing methods apply binary calibration in a pairwise
or one-vs-rest fashion.
  We propose a natively multiclass calibration method applicable to classifiers
from any model class, derived from Dirichlet distributions and generalising the
beta calibration method from binary classification. It is easily implemented
with neural nets since it is equivalent to log-transforming the uncalibrated
probabilities, followed by one linear layer and softmax. Experiments
demonstrate improved probabilistic predictions according to multiple measures
(confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of
datasets and classifiers. Parameters of the learned Dirichlet calibration map
provide insights to the biases in the uncalibrated model.

2: 
Calibration tests beyond classification
  Most supervised machine learning tasks are subject to irreducible prediction
errors. Probabilistic predictive models address this limitation by providing
probability distributions that represent a belief over plausible targets,
rather than point estimates. Such models can be a valuable tool in
decision-making under uncertainty, provided that the model output is meaningful
and interpretable. Calibrated models guarantee that the probabilistic
predictions are neither over- nor under-confident. In the machine learning
literature, different measures and statistical tests have been proposed and
studied for evaluating the calibration of classification models. For regression
problems, however, research has been focused on a weaker condition of
calibration based on predicted quantiles for real-valued targets. In this
paper, we propose the first framework that unifies calibration evaluation and
tests for general probabilistic predictive models. It applies to any such
model, including classification and regression models of arbitrary dimension.
Furthermore, the framework generalizes existing measures and provides a more
intuitive reformulation of a recently proposed framework for calibration in
multi-class classification. In particular, we reformulate and generalize the
kernel calibration error, its estimators, and hypothesis tests using
scalar-valued kernels, and evaluate the calibration of real-valued regression
problems.

3: 
Evaluating probabilistic classifiers: Reliability diagrams and score
  decompositions revisited
  A probability forecast or probabilistic classifier is reliable or calibrated
if the predicted probabilities are matched by ex post observed frequencies, as
examined visually in reliability diagrams. The classical binning and counting
approach to plotting reliability diagrams has been hampered by a lack of
stability under unavoidable, ad hoc implementation decisions. Here we introduce
the CORP approach, which generates provably statistically Consistent, Optimally
binned, and Reproducible reliability diagrams in an automated way. CORP is
based on non-parametric isotonic regression and implemented via the
Pool-adjacent-violators (PAV) algorithm - essentially, the CORP reliability
diagram shows the graph of the PAV- (re)calibrated forecast probabilities. The
CORP approach allows for uncertainty quantification via either resampling
techniques or asymptotic theory, furnishes a new numerical measure of
miscalibration, and provides a CORP based Brier score decomposition that
generalizes to any proper scoring rule. We anticipate that judicious uses of
the PAV algorithm yield improved tools for diagnostics and inference for a very
wide range of statistical and machine learning methods.

4: 
Estimating Expected Calibration Errors
  Uncertainty in probabilistic classifiers predictions is a key concern when
models are used to support human decision making, in broader probabilistic
pipelines or when sensitive automatic decisions have to be taken. Studies have
shown that most models are not intrinsically well calibrated, meaning that
their decision scores are not consistent with posterior probabilities. Hence
being able to calibrate these models, or enforce calibration while learning
them, has regained interest in recent literature. In this context, properly
assessing calibration is paramount to quantify new contributions tackling
calibration. However, there is room for improvement for commonly used metrics
and evaluation of calibration could benefit from deeper analyses. Thus this
paper focuses on the empirical evaluation of calibration metrics in the context
of classification. More specifically it evaluates different estimators of the
Expected Calibration Error ($ECE$), amongst which legacy estimators and some
novel ones, proposed in this paper. We build an empirical procedure to quantify
the quality of these $ECE$ estimators, and use it to decide which estimator
should be used in practice for different settings.

5: 
Local Calibration: Metrics and Recalibration
  Probabilistic classifiers output confidence scores along with their
predictions, and these confidence scores should be calibrated, i.e., they
should reflect the reliability of the prediction. Confidence scores that
minimize standard metrics such as the expected calibration error (ECE)
accurately measure the reliability on average across the entire population.
However, it is in general impossible to measure the reliability of an
individual prediction. In this work, we propose the local calibration error
(LCE) to span the gap between average and individual reliability. For each
individual prediction, the LCE measures the average reliability of a set of
similar predictions, where similarity is quantified by a kernel function on a
pretrained feature space and by a binning scheme over predicted model
confidences. We show theoretically that the LCE can be estimated
sample-efficiently from data, and empirically find that it reveals
miscalibration modes that are more fine-grained than the ECE can detect. Our
key result is a novel local recalibration method LoRe, to improve confidence
scores for individual predictions and decrease the LCE. Experimentally, we show
that our recalibration method produces more accurate confidence scores, which
improves downstream fairness and decision making on classification tasks with
both image and tabular data.

