1: 
Thank you for Attention: A survey on Attention-based Artificial Neural
  Networks for Automatic Speech Recognition
  Attention is a very popular and effective mechanism in artificial neural
network-based sequence-to-sequence models. In this survey paper, a
comprehensive review of the different attention models used in developing
automatic speech recognition systems is provided. The paper focuses on the
development and evolution of attention models for offline and streaming speech
recognition within recurrent neural network- and Transformer- based
architectures.

2: 
Should attention be all we need? The epistemic and ethical implications
  of unification in machine learning
  "Attention is all you need" has become a fundamental precept in machine
learning research. Originally designed for machine translation, transformers
and the attention mechanisms that underpin them now find success across many
problem domains. With the apparent domain-agnostic success of transformers,
many researchers are excited that similar model architectures can be
successfully deployed across diverse applications in vision, language and
beyond. We consider the benefits and risks of these waves of unification on
both epistemic and ethical fronts. On the epistemic side, we argue that many of
the arguments in favor of unification in the natural sciences fail to transfer
over to the machine learning case, or transfer over only under assumptions that
might not hold. Unification also introduces epistemic risks related to
portability, path dependency, methodological diversity, and increased
black-boxing. On the ethical side, we discuss risks emerging from epistemic
concerns, further marginalizing underrepresented perspectives, the
centralization of power, and having fewer models across more domains of
application

3: 
Grid Partitioned Attention: Efficient TransformerApproximation with
  Inductive Bias for High Resolution Detail Generation
  Attention is a general reasoning mechanism than can flexibly deal with image
information, but its memory requirements had made it so far impractical for
high resolution image generation. We present Grid Partitioned Attention (GPA),
a new approximate attention algorithm that leverages a sparse inductive bias
for higher computational and memory efficiency in image domains: queries attend
only to few keys, spatially close queries attend to close keys due to
correlations. Our paper introduces the new attention layer, analyzes its
complexity and how the trade-off between memory usage and model power can be
tuned by the hyper-parameters.We will show how such attention enables novel
deep learning architectures with copying modules that are especially useful for
conditional image generation tasks like pose morphing. Our contributions are
(i) algorithm and code1of the novel GPA layer, (ii) a novel deep
attention-copying architecture, and (iii) new state-of-the art experimental
results in human pose morphing generation benchmarks.

4: 
Deep Neural Networks on EEG Signals to Predict Auditory Attention Score
  Using Gramian Angular Difference Field
  Auditory attention is a selective type of hearing in which people focus their
attention intentionally on a specific source of a sound or spoken words whilst
ignoring or inhibiting other auditory stimuli. In some sense, the auditory
attention score of an individual shows the focus the person can have in
auditory tasks. The recent advancements in deep learning and in the
non-invasive technologies recording neural activity beg the question, can deep
learning along with technologies such as electroencephalography (EEG) be used
to predict the auditory attention score of an individual? In this paper, we
focus on this very problem of estimating a person's auditory attention level
based on their brain's electrical activity captured using 14-channeled EEG
signals. More specifically, we deal with attention estimation as a regression
problem. The work has been performed on the publicly available Phyaat dataset.
The concept of Gramian Angular Difference Field (GADF) has been used to convert
time-series EEG data into an image having 14 channels, enabling us to train
various deep learning models such as 2D CNN, 3D CNN, and convolutional
autoencoders. Their performances have been compared amongst themselves as well
as with the work done previously. Amongst the different models we tried, 2D CNN
gave the best performance. It outperformed the existing methods by a decent
margin of 0.22 mean absolute error (MAE).

5: 
Long Short-Term Attention
  Attention is an important cognition process of humans, which helps humans
concentrate on critical information during their perception and learning.
However, although many machine learning models can remember information of
data, they have no the attention mechanism. For example, the long short-term
memory (LSTM) network is able to remember sequential information, but it cannot
pay special attention to part of the sequences. In this paper, we present a
novel model called long short-term attention (LSTA), which seamlessly
integrates the attention mechanism into the inner cell of LSTM. More than
processing long short term dependencies, LSTA can focus on important
information of the sequences with the attention mechanism. Extensive
experiments demonstrate that LSTA outperforms LSTM and related models on the
sequence learning tasks.

