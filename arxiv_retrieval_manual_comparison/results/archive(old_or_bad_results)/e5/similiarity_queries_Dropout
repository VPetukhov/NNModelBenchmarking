1: 
DropFilter: Dropout for Convolutions
  Using a large number of parameters , deep neural networks have achieved
remarkable performance on computer vison and natural language processing tasks.
However the networks usually suffer from overfitting by using too much
parameters. Dropout is a widely use method to deal with overfitting. Although
dropout can significantly regularize densely connected layers in neural
networks, it leads to suboptimal results when using for convolutional layers.
To track this problem, we propose DropFilter, a new dropout method for
convolutional layers. DropFilter randomly suppresses the outputs of some
filters. Because it is observed that co-adaptions are more likely to occurs
inter filters rather than intra filters in convolutional layers. Using
DropFilter, we remarkably improve the performance of convolutional networks on
CIFAR and ImageNet.

2: 
Ising-Dropout: A Regularization Method for Training and Compression of
  Deep Neural Networks
  Overfitting is a major problem in training machine learning models,
specifically deep neural networks. This problem may be caused by imbalanced
datasets and initialization of the model parameters, which conforms the model
too closely to the training data and negatively affects the generalization
performance of the model for unseen data. The original dropout is a
regularization technique to drop hidden units randomly during training. In this
paper, we propose an adaptive technique to wisely drop the visible and hidden
units in a deep neural network using Ising energy of the network. The
preliminary results show that the proposed approach can keep the classification
performance competitive to the original network while eliminating optimization
of unnecessary network parameters in each training cycle. The dropout state of
units can also be applied to the trained (inference) model. This technique
could compress the network in terms of number of parameters up to 41.18% and
55.86% for the classification task on the MNIST and Fashion-MNIST datasets,
respectively.

3: 
Neural Network Regularization via Robust Weight Factorization
  Regularization is essential when training large neural networks. As deep
neural networks can be mathematically interpreted as universal function
approximators, they are effective at memorizing sampling noise in the training
data. This results in poor generalization to unseen data. Therefore, it is no
surprise that a new regularization technique, Dropout, was partially
responsible for the now-ubiquitous winning entry to ImageNet 2012 by the
University of Toronto. Currently, Dropout (and related methods such as
DropConnect) are the most effective means of regularizing large neural
networks. These amount to efficiently visiting a large number of related models
at training time, while aggregating them to a single predictor at test time.
The proposed FaMe model aims to apply a similar strategy, yet learns a
factorization of each weight matrix such that the factors are robust to noise.

4: 
Multi-Sample Dropout for Accelerated Training and Better Generalization
  Dropout is a simple but efficient regularization technique for achieving
better generalization of deep neural networks (DNNs); hence it is widely used
in tasks based on DNNs. During training, dropout randomly discards a portion of
the neurons to avoid overfitting. This paper presents an enhanced dropout
technique, which we call multi-sample dropout, for both accelerating training
and improving generalization over the original dropout. The original dropout
creates a randomly selected subset (called a dropout sample) from the input in
each training iteration while the multi-sample dropout creates multiple dropout
samples. The loss is calculated for each sample, and then the sample losses are
averaged to obtain the final loss. This technique can be easily implemented by
duplicating a part of the network after the dropout layer while sharing the
weights among the duplicated fully connected layers. Experimental results using
image classification tasks including ImageNet, CIFAR-10, and CIFAR-100 showed
that multi-sample dropout accelerates training. Moreover, the networks trained
using multi-sample dropout achieved lower error rates compared to networks
trained with the original dropout. The additional computation cost due to the
duplicated operations is not significant for deep convolutional networks
because most of the computation time is consumed in the convolution layers
before the dropout layer, which are not duplicated.

5: 
Curriculum Dropout
  Dropout is a very effective way of regularizing neural networks.
Stochastically "dropping out" units with a certain probability discourages
over-specific co-adaptations of feature detectors, preventing overfitting and
improving network generalization. Besides, Dropout can be interpreted as an
approximate model aggregation technique, where an exponential number of smaller
networks are averaged in order to get a more powerful ensemble. In this paper,
we show that using a fixed dropout probability during training is a suboptimal
choice. We thus propose a time scheduling for the probability of retaining
neurons in the network. This induces an adaptive regularization scheme that
smoothly increases the difficulty of the optimization problem. This idea of
"starting easy" and adaptively increasing the difficulty of the learning
problem has its roots in curriculum learning and allows one to train better
models. Indeed, we prove that our optimization strategy implements a very
general curriculum scheme, by gradually adding noise to both the input and
intermediate feature representations within the network architecture.
Experiments on seven image classification datasets and different network
architectures show that our method, named Curriculum Dropout, frequently yields
to better generalization and, at worst, performs just as well as the standard
Dropout method.

