1: 
Effects of Dataset properties on the training of GANs
  Generative Adversarial Networks are a new family of generative models,
frequently used for generating photorealistic images. The theory promises for
the GAN to eventually reach an equilibrium where generator produces pictures
indistinguishable for the training set. In practice, however, a range of
problems frequently prevents the system from reaching this equilibrium, with
training not progressing ahead due to instabilities or mode collapse. This
paper describes a series of experiments trying to identify patterns in regard
to the effect of the training set on the dynamics and eventual outcome of the
training.

2: 
A game-theoretic approach for Generative Adversarial Networks
  Generative adversarial networks (GANs) are a class of generative models,
known for producing accurate samples. The key feature of GANs is that there are
two antagonistic neural networks: the generator and the discriminator. The main
bottleneck for their implementation is that the neural networks are very hard
to train. One way to improve their performance is to design reliable algorithms
for the adversarial process. Since the training can be cast as a stochastic
Nash equilibrium problem, we rewrite it as a variational inequality and
introduce an algorithm to compute an approximate solution. Specifically, we
propose a stochastic relaxed forward-backward algorithm for GANs. We prove that
when the pseudogradient mapping of the game is monotone, we have convergence to
an exact solution or in a neighbourhood of it.

3: 
The Inductive Bias of Restricted f-GANs
  Generative adversarial networks are a novel method for statistical inference
that have achieved much empirical success; however, the factors contributing to
this success remain ill-understood. In this work, we attempt to analyze
generative adversarial learning -- that is, statistical inference as the result
of a game between a generator and a discriminator -- with the view of
understanding how it differs from classical statistical inference solutions
such as maximum likelihood inference and the method of moments.
  Specifically, we provide a theoretical characterization of the distribution
inferred by a simple form of generative adversarial learning called restricted
f-GANs -- where the discriminator is a function in a given function class, the
distribution induced by the generator is restricted to lie in a pre-specified
distribution class and the objective is similar to a variational form of the
f-divergence. A consequence of our result is that for linear KL-GANs -- that
is, when the discriminator is a linear function over some feature space and f
corresponds to the KL-divergence -- the distribution induced by the optimal
generator is neither the maximum likelihood nor the method of moments solution,
but an interesting combination of both.

4: 
Some Theoretical Properties of GANs
  Generative Adversarial Networks (GANs) are a class of generative algorithms
that have been shown to produce state-of-the art samples, especially in the
domain of image creation. The fundamental principle of GANs is to approximate
the unknown distribution of a given data set by optimizing an objective
function through an adversarial game between a family of generators and a
family of discriminators. In this paper, we offer a better theoretical
understanding of GANs by analyzing some of their mathematical and statistical
properties. We study the deep connection between the adversarial principle
underlying GANs and the Jensen-Shannon divergence, together with some
optimality characteristics of the problem. An analysis of the role of the
discriminator family via approximation arguments is also provided. In addition,
taking a statistical point of view, we study the large sample properties of the
estimated distribution and prove in particular a central limit theorem. Some of
our results are illustrated with simulated examples.

5: 
Generative Adversarial Nets: Can we generate a new dataset based on only
  one training set?
  A generative adversarial network (GAN) is a class of machine learning
frameworks designed by Goodfellow et al. in 2014. In the GAN framework, the
generative model is pitted against an adversary: a discriminative model that
learns to determine whether a sample is from the model distribution or the data
distribution. GAN generates new samples from the same distribution as the
training set. In this work, we aim to generate a new dataset that has a
different distribution from the training set. In addition, the Jensen-Shannon
divergence between the distributions of the generative and training datasets
can be controlled by some target $\delta \in [0, 1]$. Our work is motivated by
applications in generating new kinds of rice that have similar characteristics
as good rice.

