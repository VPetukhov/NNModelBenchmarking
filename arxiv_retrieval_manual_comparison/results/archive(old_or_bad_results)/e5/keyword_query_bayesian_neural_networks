1: 
Bayesian Neural Networks: Essentials
  Bayesian neural networks utilize probabilistic layers that capture
uncertainty over weights and activations, and are trained using Bayesian
inference. Since these probabilistic layers are designed to be drop-in
replacement of their deterministic counter parts, Bayesian neural networks
provide a direct and natural way to extend conventional deep neural networks to
support probabilistic deep learning. However, it is nontrivial to understand,
design and train Bayesian neural networks due to their complexities. We discuss
the essentials of Bayesian neural networks including duality (deep neural
networks, probabilistic models), approximate Bayesian inference, Bayesian
priors, Bayesian posteriors, and deep variational learning. We use TensorFlow
Probability APIs and code examples for illustration. The main problem with
Bayesian neural networks is that the architecture of deep neural networks makes
it quite redundant, and costly, to account for uncertainty for a large number
of successive layers. Hybrid Bayesian neural networks, which use few
probabilistic layers judicially positioned in the networks, provide a practical
solution.

2: 
Bayesian Neural Networks at Scale: A Performance Analysis and Pruning
  Study
  Bayesian neural Networks (BNNs) are a promising method of obtaining
statistical uncertainties for neural network predictions but with a higher
computational overhead which can limit their practical usage. This work
explores the use of high performance computing with distributed training to
address the challenges of training BNNs at scale. We present a performance and
scalability comparison of training the VGG-16 and Resnet-18 models on a
Cray-XC40 cluster. We demonstrate that network pruning can speed up inference
without accuracy loss and provide an open source software package,
{\it{BPrune}} to automate this pruning. For certain models we find that pruning
up to 80\% of the network results in only a 7.0\% loss in accuracy. With the
development of new hardware accelerators for Deep Learning, BNNs are of
considerable interest for benchmarking performance. This analysis of training a
BNN at scale outlines the limitations and benefits compared to a conventional
neural network.

3: 
Variational Neural Networks
  Bayesian Neural Networks (BNNs) provide a tool to estimate the uncertainty of
a neural network by considering a distribution over weights and sampling
different models for each input. In this paper, we propose a method for
uncertainty estimation in neural networks called Variational Neural Network
that, instead of considering a distribution over weights, generates parameters
for the output distribution of a layer by transforming its inputs with
learnable sub-layers. In uncertainty quality estimation experiments, we show
that VNNs achieve better uncertainty quality than Monte Carlo Dropout or Bayes
By Backpropagation methods.

4: 
Enhanced Bayesian Neural Networks for Macroeconomics and Finance
  We develop Bayesian neural networks (BNNs) that permit to model generic
nonlinearities and time variation for (possibly large sets of) macroeconomic
and financial variables. From a methodological point of view, we allow for a
general specification of networks that can be applied to either dense or sparse
datasets, and combines various activation functions, a possibly very large
number of neurons, and stochastic volatility (SV) for the error term. From a
computational point of view, we develop fast and efficient estimation
algorithms for the general BNNs we introduce. From an empirical point of view,
we show both with simulated data and with a set of common macro and financial
applications that our BNNs can be of practical use, particularly so for
observations in the tails of the cross-sectional or time series distributions
of the target variables.

5: 
Bayesian Neural Networks: An Introduction and Survey
  Neural Networks (NNs) have provided state-of-the-art results for many
challenging machine learning tasks such as detection, regression and
classification across the domains of computer vision, speech recognition and
natural language processing. Despite their success, they are often implemented
in a frequentist scheme, meaning they are unable to reason about uncertainty in
their predictions. This article introduces Bayesian Neural Networks (BNNs) and
the seminal research regarding their implementation. Different approximate
inference methods are compared, and used to highlight where future research can
improve on current methods.

