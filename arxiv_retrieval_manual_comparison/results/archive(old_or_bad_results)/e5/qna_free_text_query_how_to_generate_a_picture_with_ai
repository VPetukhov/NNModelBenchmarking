1: 
Text to artistic image generation
  Painting is one of the ways for people to express their ideas, but what if
people with disabilities in hands want to paint? To tackle this challenge, we
create an end-to-end solution that can generate artistic images from text
descriptions. However, due to the lack of datasets with paired text description
and artistic images, it is hard to directly train an algorithm which can create
art based on text input. To address this issue, we split our task into three
steps: (1) Generate a realistic image from a text description by using Dynamic
Memory Generative Adversarial Network (arXiv:1904.01310), (2) Classify the
image as a genre that exists in the WikiArt dataset using Resnet (arXiv:
1512.03385), (3) Select a style that is compatible with the genre and transfer
it to the generated image by using neural artistic stylization network
(arXiv:1705.06830).

2: 
FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN
  Space Optimization
  Generating images from natural language instructions is an intriguing yet
highly challenging task. We approach text-to-image generation by combining the
power of the retrained CLIP representation with an off-the-shelf image
generator (GANs), optimizing in the latent space of GAN to find images that
achieve maximum CLIP score with the given input text. Compared to traditional
methods that train generative models from text to image starting from scratch,
the CLIP+GAN approach is training-free, zero shot and can be easily customized
with different generators.
  However, optimizing CLIP score in the GAN space casts a highly challenging
optimization problem and off-the-shelf optimizers such as Adam fail to yield
satisfying results. In this work, we propose a FuseDream pipeline, which
improves the CLIP+GAN approach with three key techniques: 1) an AugCLIP score
which robustifies the CLIP objective by introducing random augmentation on
image. 2) a novel initialization and over-parameterization strategy for
optimization which allows us to efficiently navigate the non-convex landscape
in GAN space. 3) a composed generation technique which, by leveraging a novel
bi-level optimization formulation, can compose multiple images to extend the
GAN space and overcome the data-bias.
  When promoted by different input text, FuseDream can generate high-quality
images with varying objects, backgrounds, artistic styles, even novel
counterfactual concepts that do not appear in the training data of the GAN we
use. Quantitatively, the images generated by FuseDream yield top-level
Inception score and FID score on MS COCO dataset, without additional
architecture design or training. Our code is publicly available at
\url{https://github.com/gnobitab/FuseDream}.

3: 
Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning
  Challenge
  Automatically describing the content of an image is a fundamental problem in
artificial intelligence that connects computer vision and natural language
processing. In this paper, we present a generative model based on a deep
recurrent architecture that combines recent advances in computer vision and
machine translation and that can be used to generate natural sentences
describing an image. The model is trained to maximize the likelihood of the
target description sentence given the training image. Experiments on several
datasets show the accuracy of the model and the fluency of the language it
learns solely from image descriptions. Our model is often quite accurate, which
we verify both qualitatively and quantitatively. Finally, given the recent
surge of interest in this task, a competition was organized in 2015 using the
newly released COCO dataset. We describe and analyze the various improvements
we applied to our own baseline and show the resulting performance in the
competition, which we won ex-aequo with a team from Microsoft Research, and
provide an open source implementation in TensorFlow.

4: 
Leveraging Visual Question Answering to Improve Text-to-Image Synthesis
  Generating images from textual descriptions has recently attracted a lot of
interest. While current models can generate photo-realistic images of
individual objects such as birds and human faces, synthesising images with
multiple objects is still very difficult. In this paper, we propose an
effective way to combine Text-to-Image (T2I) synthesis with Visual Question
Answering (VQA) to improve the image quality and image-text alignment of
generated images by leveraging the VQA 2.0 dataset. We create additional
training samples by concatenating question and answer (QA) pairs and employ a
standard VQA model to provide the T2I model with an auxiliary learning signal.
We encourage images generated from QA pairs to look realistic and additionally
minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38
and increases the R-prec. from 83.82% to 84.79% when compared to the baseline,
which indicates that T2I synthesis can successfully be improved using a
standard VQA model.

5: 
Artificial Intelligence Generated Coins for Size Comparison
  Authors of scientific articles use coins in photographs as a size reference
for objects. For this purpose, coins are placed next to objects when taking the
photo. In this letter we propose a novel method that uses artificial
intelligence (AI) generated images of coins to provide a size reference in
photos. The newest generation is able to quickly generate realistic
high-quality images from textual descriptions. With the proposed method no
physical coin is required while taking photos. Coins can be added to photos
that contain none. Furthermore, we show how the coin motif can be matched to
the object.

