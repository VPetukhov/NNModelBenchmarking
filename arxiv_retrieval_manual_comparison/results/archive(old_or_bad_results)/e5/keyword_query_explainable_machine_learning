1: 
Explainable AI for Classification using Probabilistic Logic Inference
  The overarching goal of Explainable AI is to develop systems that not only
exhibit intelligent behaviours, but also are able to explain their rationale
and reveal insights. In explainable machine learning, methods that produce a
high level of prediction accuracy as well as transparent explanations are
valuable. In this work, we present an explainable classification method. Our
method works by first constructing a symbolic Knowledge Base from the training
data, and then performing probabilistic inferences on such Knowledge Base with
linear programming. Our approach achieves a level of learning performance
comparable to that of traditional classifiers such as random forests, support
vector machines and neural networks. It identifies decisive features that are
responsible for a classification as explanations and produces results similar
to the ones found by SHAP, a state of the art Shapley Value based method. Our
algorithms perform well on a range of synthetic and non-synthetic data sets.

2: 
Proposed Guidelines for the Responsible Use of Explainable Machine
  Learning
  Explainable machine learning (ML) enables human learning from ML, human
appeal of automated model decisions, regulatory compliance, and security audits
of ML models. Explainable ML (i.e. explainable artificial intelligence or XAI)
has been implemented in numerous open source and commercial packages and
explainable ML is also an important, mandatory, or embedded aspect of
commercial predictive modeling in industries like financial services. However,
like many technologies, explainable ML can be misused, particularly as a faulty
safeguard for harmful black-boxes, e.g. fairwashing or scaffolding, and for
other malevolent purposes like stealing models and sensitive training data. To
promote best-practice discussions for this already in-flight technology, this
short text presents internal definitions and a few examples before covering the
proposed guidelines. This text concludes with a seemingly natural argument for
the use of interpretable models and explanatory, debugging, and disparate
impact testing methods in life- or mission-critical ML systems.

3: 
Bounded logit attention: Learning to explain image classifiers
  Explainable artificial intelligence is the attempt to elucidate the workings
of systems too complex to be directly accessible to human cognition through
suitable side-information referred to as "explanations". We present a trainable
explanation module for convolutional image classifiers we call bounded logit
attention (BLA). The BLA module learns to select a subset of the convolutional
feature map for each input instance, which then serves as an explanation for
the classifier's prediction. BLA overcomes several limitations of the
instancewise feature selection method "learning to explain" (L2X) introduced by
Chen et al. (2018): 1) BLA scales to real-world sized image classification
problems, and 2) BLA offers a canonical way to learn explanations of variable
size. Due to its modularity BLA lends itself to transfer learning setups and
can also be employed as a post-hoc add-on to trained classifiers. Beyond
explainability, BLA may serve as a general purpose method for differentiable
approximation of subset selection. In a user study we find that BLA
explanations are preferred over explanations generated by the popular
(Grad-)CAM method.

4: 
FOLD-SE: An Efficient Rule-based Machine Learning Algorithm with
  Scalable Explainability
  We present FOLD-SE, an efficient, explainable machine learning algorithm for
classification tasks given tabular data containing numerical and categorical
values. FOLD-SE generates a set of default rules-essentially a stratified
normal logic program-as an (explainable) trained model. Explainability provided
by FOLD-SE is scalable, meaning that regardless of the size of the dataset, the
number of learned rules and learned literals stay quite small while good
accuracy in classification is maintained. A model with smaller number of rules
and literals is easier to understand for human beings. FOLD-SE is competitive
with state-of-the-art machine learning algorithms such as XGBoost and
Multi-Layer Perceptrons (MLP) wrt accuracy of prediction. However, unlike
XGBoost and MLP, the FOLD-SE algorithm is explainable. The FOLD-SE algorithm
builds upon our earlier work on developing the explainable FOLD-R++ machine
learning algorithm for binary classification and inherits all of its positive
features. Thus, pre-processing of the dataset, using techniques such as one-hot
encoding, is not needed. Like FOLD-R++, FOLD-SE uses prefix sum to speed up
computations resulting in FOLD-SE being an order of magnitude faster than
XGBoost and MLP in execution speed. The FOLD-SE algorithm outperforms FOLD-R++
as well as other rule-learning algorithms such as RIPPER in efficiency,
performance and scalability, especially for large datasets. A major reason for
scalable explainability of FOLD-SE is the use of a literal selection heuristics
based on Gini Impurity, as opposed to Information Gain used in FOLD-R++. A
multi-category classification version of FOLD-SE is also presented.

5: 
Expressive Explanations of DNNs by Combining Concept Analysis with ILP
  Explainable AI has emerged to be a key component for black-box machine
learning approaches in domains with a high demand for reliability or
transparency. Examples are medical assistant systems, and applications
concerned with the General Data Protection Regulation of the European Union,
which features transparency as a cornerstone. Such demands require the ability
to audit the rationale behind a classifier's decision. While visualizations are
the de facto standard of explanations, they come short in terms of
expressiveness in many ways: They cannot distinguish between different
attribute manifestations of visual features (e.g. eye open vs. closed), and
they cannot accurately describe the influence of absence of, and relations
between features. An alternative would be more expressive symbolic surrogate
models. However, these require symbolic inputs, which are not readily available
in most computer vision tasks. In this paper we investigate how to overcome
this: We use inherent features learned by the network to build a global,
expressive, verbal explanation of the rationale of a feed-forward convolutional
deep neural network (DNN). The semantics of the features are mined by a concept
analysis approach trained on a set of human understandable visual concepts. The
explanation is found by an Inductive Logic Programming (ILP) method and
presented as first-order rules. We show that our explanation is faithful to the
original black-box model.
  The code for our experiments is available at
https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.

