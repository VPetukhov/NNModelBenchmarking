1: 
Scale Efficiently: Insights from Pre-training and Fine-tuning
  Transformers
  There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.

2: 
Staged Training for Transformer Language Models
  The current standard approach to scaling transformer language models trains
each model size from a different random initialization. As an alternative, we
consider a staged training setup that begins with a small model and
incrementally increases the amount of compute used for training by applying a
"growth operator" to increase the model depth and width. By initializing each
stage with the output of the previous one, the training process effectively
re-uses the compute from prior stages and becomes more efficient. Our growth
operators each take as input the entire training state (including model
parameters, optimizer state, learning rate schedule, etc.) and output a new
training state from which training continues. We identify two important
properties of these growth operators, namely that they preserve both the loss
and the "training dynamics" after applying the operator. While the
loss-preserving property has been discussed previously, to the best of our
knowledge this work is the first to identify the importance of preserving the
training dynamics (the rate of decrease of the loss during training). To find
the optimal schedule for stages, we use the scaling laws from (Kaplan et al.,
2020) to find a precise schedule that gives the most compute saving by starting
a new stage when training efficiency starts decreasing. We empirically validate
our growth operators and staged training for autoregressive language models,
showing up to 22% compute savings compared to a strong baseline trained from
scratch. Our code is available at https://github.com/allenai/staged-training.

3: 
Layered gradient accumulation and modular pipeline parallelism: fast and
  efficient training of large language models
  The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.
  In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.

4: 
Podracer architectures for scalable Reinforcement Learning
  Supporting state-of-the-art AI research requires balancing rapid prototyping,
ease of use, and quick iteration, with the ability to deploy experiments at a
scale traditionally associated with production systems.Deep learning frameworks
such as TensorFlow, PyTorch and JAX allow users to transparently make use of
accelerators, such as TPUs and GPUs, to offload the more computationally
intensive parts of training and inference in modern deep learning systems.
Popular training pipelines that use these frameworks for deep learning
typically focus on (un-)supervised learning. How to best train reinforcement
learning (RL) agents at scale is still an active research area. In this report
we argue that TPUs are particularly well suited for training RL agents in a
scalable, efficient and reproducible way. Specifically we describe two
architectures designed to make the best use of the resources available on a TPU
Pod (a special configuration in a Google data center that features multiple TPU
devices connected to each other by extremely low latency communication
channels).

5: 
Reducing Activation Recomputation in Large Transformer Models
  Training large transformer models is one of the most important computational
challenges of modern AI. In this paper, we show how to significantly accelerate
training of large transformer models by reducing activation recomputation.
Activation recomputation is commonly used to work around memory capacity
constraints. Rather than storing activations for backpropagation, they are
traditionally recomputed, which saves memory but adds redundant compute. In
this work, we show most of this redundant compute is unnecessary because we can
reduce memory consumption sufficiently without it. We present two novel yet
very simple techniques: sequence parallelism and selective activation
recomputation. In conjunction with tensor parallelism, these techniques almost
eliminate the need to recompute activations. We evaluate our approach on
language models up to one trillion parameters in scale and show that our method
reduces activation memory by 5x, while reducing execution time overhead from
activation recomputation by over 90%. For example, when training a 530B
parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops
Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using
recomputation. Our implementation will be available in both Megatron-LM and
NeMo-Megatron.

